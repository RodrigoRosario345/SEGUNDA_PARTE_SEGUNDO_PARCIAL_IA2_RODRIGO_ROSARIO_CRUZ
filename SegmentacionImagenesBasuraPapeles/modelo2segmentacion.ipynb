{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\SAM_MaskGenerator_Test-master\\SegmentacionImagenesBasuraPapeles\\modelo2segmentacion.ipynb Celda 1\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\SAM_MaskGenerator_Test-master\\SegmentacionImagenesBasuraPapeles\\modelo2segmentacion.ipynb Celda 1\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m segmentation \u001b[39m=\u001b[39m segment_image(model, image_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m draw_bounding_boxes(image, bounding_boxes, segmentation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mImage with Bounding Boxes\u001b[39m\u001b[39m\"\u001b[39m, image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\SAM_MaskGenerator_Test-master\\SegmentacionImagenesBasuraPapeles\\modelo2segmentacion.ipynb Celda 1\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(image, (x, y), (x \u001b[39m+\u001b[39m w, y \u001b[39m+\u001b[39m h), (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m cropped_segmentation \u001b[39m=\u001b[39m segmentation[y:y\u001b[39m+\u001b[39mh, x:x\u001b[39m+\u001b[39mw]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39mSegmentation\u001b[39;49m\u001b[39m\"\u001b[39;49m, cropped_segmentation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/SAM_MaskGenerator_Test-master/SegmentacionImagenesBasuraPapeles/modelo2segmentacion.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "import numpy as np\n",
    "\n",
    "def load_model():\n",
    "    model = deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def segment_image(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)['out']\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return predicted.squeeze().cpu().numpy()\n",
    "\n",
    "def draw_bounding_boxes(image, bounding_boxes, segmentation):\n",
    "    for bbox in bounding_boxes:\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cropped_segmentation = segmentation[y:y+h, x:x+w]\n",
    "        cv2.imshow(\"Segmentation\", cropped_segmentation)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "def main():\n",
    "    image_path = \"image_examples/prueba.jpg\"\n",
    "    bounding_boxes = np.array([[15, 400, 269, 525], [300, 40, 435, 395]])\n",
    "\n",
    "    model = load_model()\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    segmentation = segment_image(model, image_tensor)\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    draw_bounding_boxes(image, bounding_boxes, segmentation)\n",
    "\n",
    "    cv2.imshow(\"Image with Bounding Boxes\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
